\chapter{Conclusions} \label{chap:conclusions}

% One frequent activity carried out in scientific research is comparing entities between themselves in order to find patterns, to transfer knowledge from one entity to another and to organise the ever increasing archives of knowledge that power the process of scientific discovery. But comparing entities is non-trivial, as a significant part of these entities is not mathematically representable in a way that warrants numeric analysis. In biomedical informatics, for example, calculation of similarity between entities has therefore resorted to making these entities more amenable to computer treatment, \eg by representing proteins as their amino-acid sequence or as their three dimensional structure, and by representing chemical compounds as a graph corresponding to its chemical structure. However, some entities cannot easily be represented in this type of format (examples include clinical records and literature). In fact, even those representations of proteins and chemical compounds are sometimes inadequate, since some of the details are lost. For instance, there are proteins with similar amino-acid sequences that perform different roles in the cell; some molecules with similar structures have different effects on the body; \etc. These shortcomings, and the desire to be as precise and accurate as possible, led the biomedical informatics community to look into knowledge representation techniques to more faithfully represent their resources in machine-readable formats that can properly reflect the real-life entities. This step includes
% \begin{paralist}
%     \item developing knowledge representation artefacts, such as ontologies, to
%     \item annotate the resources.
% \end{paralist}
% In turn, this allows techniques that rely on the annotations, such as semantic similarity, to make computational use of the \emph{meaning} that these annotations yield to the resources.

Semantic similarity in the biomedical domain has been used to compare entities like proteins, chemical compounds and metabolic pathways. However, biomedical knowledge is intrinsically multidisciplinary: for example, metabolic pathways are related with chemical compounds, proteins, and even other types of concepts such as diseases; epidemiological resources are related to concepts from a wide range of domains, like diseases, symptoms, environmental conditions, \etc. Comparing these multidisciplinary entities based on their semantics is a problem that, until now, had not yet been tackled.


\section{Summary of contributions} \label{sec:conclusions/contributions}

The main objective of my PhD work was to research methods to handle that problem, \ie I studied and offered solutions to the issue of multi-domain semantic similarity measures. The main hypothesis presented in this document is that multi-domain semantic similarity measures can be constructed by lifting existing single-ontology algorithms into the multidisciplinary case. The main result work was, indeed, the empirical proof of this hypothesis by validating the use of multi-domain measures in several biomedical datasets (resources in a epidemiology marketplace, metabolic pathways, and mathematical models of biochemical systems).

While the quantitative results are not best understood in numeric format (in fact most of the results in \chpref{chap:multidomain} are presented as graphs), it is visible in all those graphs that multi-domain semantic similarity almost always outperforms the single-ontology baselines, with very few exceptions: \eg a baseline using a wide-coverage ontology, such as \ontology{NCIt}, has the same performance as the multi-domain approaches, and the \ontology{GO} baseline also has a high performance given that the amount of annotations from this ontology far surpasses the amount of annotations in the other ontologies. In all cases, however, the \textbf{Integrative} approach to multi-domain semantic similarity always outperforms the best single-ontology baselines. This means that, in fact, the knowledge encoded in the annotations from one domain complements the knowledge encoded in the annotations from the other domains, thus leading to the idea that, in fact, technology that deals with several domains simultaneously needs to be developed in order to properly explore all the information contained in multidisciplinary resources.

As in all scientific endeavours, getting to prove an interesting and useful statement requires that a significant amount of work be performed ``under the hood''. In fact, when I started my work, few to no publications existed that dealt with the issue of multidisciplinarity in the biomedical domain and, as far as I was able to ascertain, in any other scientific field. For this reason, the methodology I set for myself achieved intermediate results that were also essential to support the hypothesis.

One the one hand, I created a hierarchy of semantic similarity validation strategies~(\chpref{chap:validation}). This hierarchy, created with a reproducible method, can be used to categorise semantic similarity measures according to the method used by their creators to perform validation, which not only helps categorise the measures, but also allows researchers interested in using semantic similarity to choose a measure that has been shown to have a high performance in the type of problems at hand.

On the other hand, I also contributed to the current panorama in single-ontology semantic similarity~(\chpref{chap:enhancements}). While this was not a requisite for the ultimate proof of the hypothesis, it provided a first step towards including formal logic constructions in the semantic measures existing today: since my approach to multi-domain similarity is based on pre-existing single-ontology measures, any improvement made to the existing measures will also have a positive impact on the overall performance of the multi-domain measures. In fact, one of the measures developed in this context, $\ferreira$, is able to calculate relatedness based on on all the properties associated with the concepts of the ontology, not just its class-subclass hierarchy. The success of this measure in the single-ontology world (where it was used to determine whether pairs of anatomical concepts are implicated in the same disease) is reflected in the multidisciplinary datasets, since using it as the base of the multi-domain measure increases the performance with respect to purely similarity measures (such as $\sim[Resnik]$).

Furthermore, to properly study multi-domain measures, I had to collect multidisciplinary data, which resulted in a collection of three datasets~(\chpref{chap:data}). The first dataset comes from the epidemiology field and contains references to epidemiological articles annotated with concepts from a network of epidemiology-related ontologies. The second dataset contains metabolic pathways annotated with the metabolites that are converted in the pathway and the proteins responsible for catalysing those reactions, as well as the diseases associated with the malfunctioning of the pathways and the drugs that affect them. The third dataset contains mathematical models of biological systems, again annotated with the intervening metabolites and proteins, but also with the anatomical places where those systems are located and the physical quantities measured in those mathematical models.

Finally, given the large amount of information that the measures deal with, I had to develop software mechanisms to cope with the size of biomedical ontologies, namely \owlsql\ and \mossy, which provide an automatic way to run semantic similarity calculations both faster and in a more reproducible manner. As such, this part of my work was more technical than exploratory.

I also participated in several other activities in parallel to the ones described here, which, although not directly related to semantic similarity, were essential to my understanding of how this technique can assist the advancement of science. \appref{app:auxiliary-projects} mentions my work in the Epidemic Marketplace (especially in developing the Network of Epidemiology-Related Ontologies), text-mining, and ontology matching.

As explained in \chpref{chap:intro}, the results obtained in this work were entirely directed at the biomedical domain, and as such only this area of research directly benefits from these results. However, none of the methods I developed is specific for this area of research and can be adapted to work in other domains. For example, the amount of information living in the semantic web (see \secref{sec:concepts/semantic-web}) is also increasing and we will soon need the power of computational methods to deal with that amount of data. From a technical point of view, adapting the presented methods to this area is trivial, as the only requirement is that the necessary ontologies exist and are included in an \owlsql\ database.


\section{Some shortcomings} \label{sec:conclusions/criticism}

Not unlike other scientific endeavours, the work that I performed during my PhD has its own shortcomings and weaknesses. Here I present a few, together with possible avenues to solve them.

I did not find evidence to suggest that using cross-references increases the performance of semantic similarity. I originally hypothesised that semantic similarity measures that use the cross-references would outperform the same measure but without using such links. The results in \secref{sec:multidomain/results} indicate that this is not the case. This can be due to the small number of such references, which are bound to increase both in quantity and in quality as the inter-domain knowledge representation efforts increase. While the long-term solution involves waiting for the proper knowledge to be encoded in machine-readable formats, a short-term solution to this problem is to use external sources of information to find inter-domain links between the concepts in the ontologies of relevance. For example, I propose using text-mining techniques to find co-occurrences of anatomical terms and disease names in scientific literature. Pairs of concepts often mentioned together can then be inferred to have some sort of relationship. Furthermore, we can explore the frequency with which such co-occurrences appear in a corpus to assign a strength for these relationships.

Another weakness of this work is that the results obtained to validate the the multi-domain semantic similarity approaches are not representative of real-world scenarios. For instance, using semantic similarity to classify metabolic pathways in groups that have already been assigned manually shows that the measures are sound, but does not actually produce new knowledge. Additionally, it would be useful to harness the power of online data repository users to establish whether new annotations predicted from existing ones are correct.

A third problem with the results is that, in some cases, performance indicators are not as high as was desired. For example, in the biomodels dataset, the best performance was achieved using $\ferreira$ as the group-wise single-ontology semantic similarity with the integrative approach, but these results show a Spearman's rank correlation coefficient of about~$0.35$. To solve this issue, I expect that tuning the measures will account to an increase in the performance indicators. For example, $\ferreira$ is a measure that can be tuned with respect to the weights assigned to each property. Running experiments where the weights are changed according to some criteria may be useful to increase the performance. Note, however, that the small absolute correlation does not invalidate the conclusion that multi-domain measures outperform single-ontology ones: this hypothesis still holds, and in fact by a large margin (see \eg \figref{fig:biomodels-ferreira}).

% The last problem is a technical one. Although \owlsql\ and \mossy\ were developed to deal with large volumes of data, time performance is not very convenient. For example, comparing all $269$~metabolic pathways with each other, using $\ferreira$ and the integrative approach takes more than $10$~days in a 3GHz CPU. One of the reasons for this is that \mossy\ is synchronous, and therefore is unable to use more than one CPU for its calculations. Another problem is that this program makes high use of interprocess communication with MySQL, which is where the bottleneck of the processing is. Furthermore, \mossy\ is programmed in Python, a relatively slow interpreted language. However, an earlier draft implementation was originally written in Java, and the execution times were not significantly shorted (data, unfortunately, unavailable), which means that Python's slowness does not contribute significantly to the overall slow execution of \mossy.

It is pertinent to notice that these issues can all be solved in time. In fact, as future work, I propose that these are exactly the next steps to develop a fully cohesive multi-domain semantic similarity theory.


\section{Future work} \label{sec:conclusions/future}

Besides the points raised in the previous section, there are at least three more aspects that I feel would greatly improve the overall panorama in multi-domain semantic similarity.

I would like to explore the idea of calculating the ``relevance'' of concepts to assist the computation of $\ferreira$. This relatedness measure first finds the semantic neighbourhood of a concept~$c$ by traversing the properties between concepts and building a graph centred around~$c$, and doing it recursively. This semantic neighbourhood is, therefore, exponential on the number of ``layers'' that we want to capture (although in practice the number of layers is relatively small compared to the size of the ontology). To mitigate the effort of this step, we could devise an algorithm that decides whether a concept is relevant, reducing the size of the neighbourhood and therefore the execution time. This relevance measure could also benefit other measures, such as $\sim[GIC]$, which must know for each concept the set of its superclasses; by storing only the relevant superclasses, we could improve the speed, memory requirements, and accuracy of this measure. I have already obtained some preliminary results on this idea which suggest in fact that accuracy increases when only a fraction of superclasses is considered. Further studies need to be developed, however.

I would also like to understand the effect of the aggregation mechanism that is used to compare lists of concepts with a concept-wise semantic similarity (see \secref{sec:sota/annotated}). For example, to use $\sim[Resnik]$ as a group-wise semantic similarity, I used the Best Match Average (BMA) approach to convert the matrix of similarity values into a single value. BMA does this by finding in each row and column of this matrix the highest value and then averaging the values (see \figref{fig:bma}), but a possible alternative would be to use, from each row and column, more than one value using, for example, the T-conorm idea proposed by \citet{Lehmann2012a} and already explored in \secref{sec:sota/annotated}.

As mentioned in \chpref{chap:validation}, I also intend to create a tool that assists semantic similarity developers in setting up a validation step to their own measures, based on the hierarchy described in that chapter; furthermore, the hierarchy itself can be included within one of the already existing ontologies, both as a means to standardise it and as a way to motivate its use by the community and its future extension to accommodate other domains.

Finally, as a matter of speed, I would like to explore and modify the current implementation of \mossy\ so that it can use asynchronous programming and so that it can reduce its dependency on the underlying MySQL database, for example by storing frequently requested information in a local cache. I expect that such modifications would greatly increase the speed of execution, particularly on a multi-core machine.


\section{Last thoughts} \label{sec:conclusions/thoughts}

It is undeniable that science can no longer be performed by human mind alone. The data being produced today is so extensive in size that it has become impossible to be aware of all of it without the assistance of computerised systems that crunch the information and hand it over to the scientists in more manageable formats. Semantic similarity is but one aspect of this whole automatic pipeline, a cog in the machine that intends to assist scientific progress.

My contribution, as most contributions in today's scientific community, is but a tiny bump on the frontier of human knowledge, but it so happens that, along with the millions of other scientists working towards knowledge discovery, it is building and improving our own understanding of ourselves and our world. In this sense, I believe my work is a small but steady step towards the future of science.
